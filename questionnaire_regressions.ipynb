{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_project import *\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_item(item):\n",
    "    return questionnaire_items[questionnaire_items['item']==item]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the use of factor analysis: regressions w/ individual questionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS\n",
    "- each questionnaire individually\n",
    "- questionnaires together\n",
    "    - also: select questionnaires on first dataset, then replicate relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_labels(ques_prefix):\n",
    "    return [c for c in data.columns if (c.split('_')[0] == ques_prefix) & ('score' not in c) & ('_att' not in c)]\n",
    "\n",
    "# put togehter int a dictionary\n",
    "behs = ['affil_mean_mean_z', 'power_mean_mean_z', 'pov_2d_dist_mean_mean_z']\n",
    "ques = ['oci_score', 'sds_score', 'aes_score', 'sss_score', 'lsas_av_score', 'apdis_score', 'zbpd_score', 'bapq_score']\n",
    "for q in ques: data[f'{q}_z'] = zscore_masked(data[q].values) # zscore the questionnaire scores\n",
    "sample_dict = reset_sample_dict(data)\n",
    "ques_scores = [f'{q}_z' for q in ques]\n",
    "\n",
    "ques_prefix = [q.split('_')[0] for q in ques]\n",
    "ques_dict   = {'scores': ques_scores, 'items': flatten_lists([get_item_labels(qp) for qp in ques_prefix])}\n",
    "\n",
    "# run ols\n",
    "ols_df = pd.DataFrame(columns=['sample', 'predictor_type', 'self-report', 'behavior', 'beta', 'p'])\n",
    "for sample in ['Initial', 'Replication', 'Combined']:\n",
    "    df = sample_dict[sample]\n",
    "    for b in behs:  \n",
    "        for qtype, qs in ques_dict.items():\n",
    "            \n",
    "            # behavior ~ single score/item\n",
    "            for q in qs:\n",
    "                ols = run_ols([q], b, df, covariates=demo_controls)[1]\n",
    "                ols_df.loc[len(ols_df), :] = [sample, f'independent_{qtype}', q, b, ols.params[q], ols.pvalues[q]] \n",
    "\n",
    "            # behavior ~ all scores/items\n",
    "            ols = run_ols(qs, b, df, covariates=demo_controls)[1]\n",
    "            for q in qs:\n",
    "                ols_df.loc[len(ols_df), :] = [sample, f'combined_{qtype}', q, b, ols.params[q], ols.pvalues[q]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many items are reverse scored, so these should all be: higher value means more symptom\n",
    "item_df = ols_df[(ols_df['sample'] == 'Initial') \n",
    "                 & (ols_df['predictor_type'] == 'combined_items') \n",
    "                 & (ols_df['behavior'] == 'pov_2d_dist_mean_mean_z')]\n",
    "item_df.sort_values(by=['sample','p'], inplace=True)\n",
    "item_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# find top items\n",
    "item_df_ = item_df.iloc[:10]\n",
    "for qp in ques_prefix: \n",
    "    items = item_df_[item_df_['self-report'].str.contains(qp)]['self-report']\n",
    "    print(f'{qp}: {items.values}')\n",
    "\n",
    "# find the text\n",
    "for i, item in enumerate(item_df['self-report']):\n",
    "    try: \n",
    "        item_text = questionnaire_items[questionnaire_items['item'] == item]['text'].values[0]\n",
    "        item_df.loc[i, 'item_text'] = item_text\n",
    "    except: \n",
    "        item_df.loc[i, 'item_text'] = 'missing'\n",
    "item_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights, colors, title, ax):\n",
    "    weights = weights.astype(float)\n",
    "    w_max = np.round(np.max(np.abs(weights)), 1)\n",
    "    ax.bar(np.arange(len(weights)), weights, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    #ax.set_ylim(-w_max-(0.005*w_max), w_max+(0.005*w_max)) \n",
    "\n",
    "behs = ['pov_2d_dist_mean_mean_z']\n",
    "\n",
    "# for plotting\n",
    "colors = ['red', 'blue', 'purple', 'green', 'lavender', 'grey', 'fuchsia', 'orange', 'dodgerblue', \n",
    "          'yellow', 'orchid', 'indigo', 'aqua','palegreen', 'silver', 'plum', 'fuchsia', 'coral',\n",
    "          'gold', 'pink','slategray', 'forestgreen','peachpuff','honeydew','brown','olivedrab',\n",
    "          'darkturquoise', 'tan', 'springgreen', 'mintcream','navajowhite','chocolate','lightblue','chartreuse',\n",
    "          'lime','yellowgreen','khaki','gold','teal','tomato']\n",
    "colors_items = [colors[ques_prefix.index(item.split('_')[0])] for item in ques_items]\n",
    "\n",
    "for sample in ['Combined']:\n",
    "    ols_res = ols_df[ols_df['sample'] == sample]\n",
    "\n",
    "    fig, axs = plt.subplots(len(behs), 3, figsize=(15, 3*len(behs)), gridspec_kw={'width_ratios': [3, 3, 20]})\n",
    "    fig.suptitle(f'{sample} sample', fontsize=18, y=1.01)\n",
    "    if len(behs) == 1:\n",
    "        ax1, ax2, ax3 = axs[0], axs[1], axs[2]\n",
    "    else:\n",
    "        ax1, ax2, ax3 = axs[i,0], axs[i,1], axs[i,2]\n",
    "\n",
    "    for i, beh in enumerate(behs): \n",
    "\n",
    "        # questionnaire ~ behavior\n",
    "        behav = ols_res[(ols_res['predictor_type'] == 'independent') & (ols_res['predicted'] == beh)]\n",
    "        plot_weights(behav['beta'].values, colors, title='Scores independent', ax=ax1)\n",
    "        \n",
    "        # behavior ~ questionnaires\n",
    "        behav = ols_res[(ols_res['predictor_type'] == 'covariate_scores') & (ols_res['predicted'] == beh)]\n",
    "        plot_weights(behav['beta'].values, colors, title='Scores covariates', ax=ax2)\n",
    "        \n",
    "        # behavior ~ questionnaire items\n",
    "        behav = ols_res[(ols_res['predictor_type'] == 'covariate_items') & (ols_res['predicted'] == beh)]\n",
    "        plot_weights(behav['beta'].values, colors_items, title='Items covariates', ax=ax3)\n",
    "\n",
    "    # overall legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    patches = [mpatches.Patch(facecolor=inst[0], edgecolor='black', label=inst[1]) for inst in zip(colors, ques)]  \n",
    "    ax3.legend(title='', loc='upper right', handles=patches,  \n",
    "                    title_fontsize=13, fontsize=8,\n",
    "                    frameon=False, bbox_to_anchor=(1.15, 1), borderaxespad=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
